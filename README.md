# AI-education-Chinese
Chinese AI education blog for Generative AI new deep dive technical blog
CPU能否用于推理
大家也看到了最近发生的新闻，A800和H800都不让在国内卖了，甚至4090都有可能被禁（看趋势应该会被放开，对于纯消费卡的限制也确实无厘头，虽然4090用来训练，调整好超参是真不错）

       晚上和朋友吃饭，席间一个朋友问起，为什么不能用CPU来跑大模型呢，如果训练不行的话，难道推理也不可以吗？其实这是一个很好的问题，但是和训练和推理其实没啥关系，这问题如果延伸下去是要触及到CPU和GPU计算的本质

        直接上图

Image

       然后说结论:

       CPU中缓存单元大概占50%，控制单元25%，运算单元25%

        GPU中缓存单元大概占5%，控制单元5%，运算单元90%

      其实是需求决定了架构，CPU为了完成复杂的任务调度，多进程/线程协同所以它必须要有更大的缓存和控制单元来追求上下文，GPU不需要，一个字就是"算", 超强的并行能力数千甚至数万核，别的其实它也干不了

      那深度学习为什么需要GPU，先说结论，目前所有的深度学习求解，并不是数学问题（完美解），而是一个近似值的求解过程，通过梯度下降和反向传播不断的优化整个模型特征的权重值来得到近乎完美的模型（永远也不可能完美）。

       所有的深度学习模型表达其实都可以用简单的线性回归来表示，无论RNN还是Transformer，其实都是变体，我们看一个简单的式子：

        

Image

      相当于左右的这一层的每个要和右边的这一层的所有节点全部进行一次乘法，可以写成：

Image

       实际上深度学习的计算就这么简单，说白了就是矩阵之间乘法，每次一个矩阵乘法实际上等价于input向量和parameter之间就完成了一次加法一次乘法，记作一次MAC, 也可以记作2次FLOPs操作。

       其实看到这里细心的读者就会发现，在matmul(A,B)的情况下，所有的操作是独立的，每个计算都不互相干预，也不需要什么上下文，所以我们可以用GPU的超多核的并行能力，来对矩阵计算进行加速，当然开启TensorCore和其他一些feature，计算会更加的迅速。

        回到这篇文章问题的本身，CPU适不适合做推理，其实早期CPU的并行能力上无法和GPU相比，目前我理解的CPU最大的core数量，是Intel Sierra Forest 可以288core（没上市），但是如果开启了AMX的能力（单AMX单元与单AVX单元的每时钟周期的算力提高了16倍），这个问题就出现了变化；一个cycle里开启AMX指令集的CPU可以进行2048次int8的计算，假设 boost能到3Ghz左右，也就是1秒钟它可以操作3*10^9次运算，他的一个核可以实现1秒钟理论上可以 计算6144*10^9 FLOPs, 等于6TFOPs, 那么288核可以实现1728TFLOPs 的int8能力, 那么一个普通的推理卡A10大概得int8算力是多少呢?



Image

      Tesnor Core的情况下是500，开启TensorRT会更快，从简单的对比数据来看，1728>500, 我们的前提是288核可以实现支持每核3Ghz boost,且开启AMX的情况下，目前市面上大多数的Cloud IaaS不支持开启AMX的指令集，那么这个每cycle处理矩阵的数值就要从2048变成512（AVX512的SIMD），就剩下432TFLOPs了，但是Cloud PaaS的AI服务，我了解到很多PaaS推理是用了AMX的能力的。

      除了算力我们也需要考虑一下性价比，目前市面上没有Sierra Forest的机型可以参考，这里找了一个96core的机型（价格仅供参考就是看个比例而已）一小时6美金，288core的话简单乘以3，大概18美金一小时。

Image

       A10 需要多少钱呢？一小时要4美金
Image

      如果纯从数值上来看TFLOPs能力3.5倍，价格上是4.5倍, 但是别忘了内存，A10只有24G内存，而即便不乘以3，CPU的对照组也有将近700G的内存，我们知道在推理中加速的一个最简单的方法就是cacheKV对，你的内存越大，越可以cache住更多的KV对，相对来讲快速命中的几率也就越大，典型的空间换算力业务, 此外我内存大意味着有更大batchsize，可以通过调整超参获得更好的吞吐和性能的最优比。
      所以结论是什么？

      光看字面的TLOPs数据似乎即使用CPU来进行推理也不是很有优势，是不是可以说在深度学习的情况下，CPU就没用了呢？这个当然不是，因为GPU所需要的训练和推理数据和GPU所在的节点的调度逻辑都要由CPU来执行，另外很多GPU服务器，尤其是云服务器，是配置了很多的CPU资源的，如果不涉及到训练数据进GPU之前的预处理，其实在很多时间里是空载状态，在这个场景下为了提升整体服务器的算力水平，把它榨干，是可以考虑有些推理任务放置在CPU上运行，做混合推理的，增加并行度也意味着提升整体的处理速度；另外一个很重要的应用就是Agent，我们知道当模型尤其小型模型在本来就容易产生幻觉的情况下，quantized以后，比如int8，就更容易产生幻觉和无法很好的驾驭下游任务，这个时候我们会采用RAG和Agent等操作，通过外部tools和内部逻辑去解决这些问题，进而精确的匹配下游任务，这些复杂的任务调度，就是GPU无法胜任的了，全部需要CPU来进行处理；本文着力点其实在推理，实际上GPU训练的领域CPU的大内存有很大的作用，这个先卖个关子，以后讲。

       从事务发展的角度上，可以预见CPU的厂商一定会更加重视CPU在AI方面的能力，甚至消费级的CPU都要和NPU有整合，大家可以搜一下Meteor Lake，其实Apple的A17已经在这么做了，所以不远的将来，云端和消费端的AI CPU或者CPU携NPU会对GPU的市场发出强有力的挑战。

       觉得写的还行的读者大爷请别吝惜你的赞和转发，谢谢！
